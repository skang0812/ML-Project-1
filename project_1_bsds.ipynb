{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/skang0812/ML-Project-1/blob/main/project_1_bsds.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3647abcb",
      "metadata": {
        "id": "3647abcb"
      },
      "source": [
        "## Project 1\n",
        "\n",
        "The goal of the first project is to do some wrangling, EDA, and visualization, and generate sequences of values. We will focus on:\n",
        "\n",
        "- CDC National Health and Nutritional Examination Survey (NHANES, 1999-2000): https://wwwn.cdc.gov/nchs/nhanes/continuousnhanes/default.aspx?BeginYear=1999\n",
        "- CDC Linked Mortality File (LMF, 1999-2000): https://www.cdc.gov/nchs/data-linkage/mortality-public.htm\n",
        "\n",
        "NHANES is a rich panel dataset on health and behavior, collected bi-yearly from around 1999 to now. We will focus on the 1999 wave, because that has the largest follow-up window, providing us with the richest mortality data. The mortality data is provided by the CDC Linked Mortality File.\n",
        "\n",
        "The purpose of the project is to use $k$-NN to predict who dies (hard or soft classification) and how long they live (regression).\n",
        "\n",
        "### Part 1: Wrangling and EDA (40/100 pts)\n",
        "\n",
        "First, go to the NHANES and LMF web sites and familiarize yourself with the data sources. Download codebooks. Think about what resources are available. The CDC Linked Mortality File is somewhat of a pain to work with, so I have pre-cleaned it for you. It is available at httts://github.com/ds4e/undergraduate_ml_assignments in the data folder, as `lmf_parsed.cav`. From the CDC LMF web page, get the SAS program to load the data; it is the real codebook.\n",
        "\n",
        "Second, download the demographic data for the 1999--2000 wave from the NHANES page. You can use the following code chunk to merge the LMF and DEMO data:\n",
        "\n",
        "``` python\n",
        "import pandas as pd\n",
        "mdf = pd.read_csv('linked_mortality_file_1999_2000.csv') # Load mortality file\n",
        "print( mdf.head() )\n",
        "gdf = pd.read_sas(\"DEMO.xpt\", format=\"xport\") # Load demographics file\n",
        "print( gdf.head() )\n",
        "df = gdf.merge(mdf, on=\"SEQN\", how=\"inner\") # Merge mortality and demographics on SEQN variable\n",
        "```\n",
        "\n",
        "Third, the variables `ELIGSTAT`, `MORTSTAT`, `PERMTH_INT`, and `RIDAGEEX` are particularly important. Look them up in the documentation and clearly describe them. (5/100 pts.)\n",
        "\n",
        "Second, the goal of the project is to use whatever demographic, behavioral, and health data you like to predict mortality (`MORTSTAT`) and life expectancy (`PERMTH_INT`). Go to the NHANES 1999--2000 web page and select your data and download it. Clearly explain your rationale for selecting these data. Use `.merge` to combine your data into one complete dataframe. Document missing values. (5/100 pts)\n",
        "\n",
        "Third, do basic EDA and visualization of the key variables. Are any important variables skewed? Are there outliers? How correlated are pairs of variables? Do pairs of categorical variables exhibit interesting patterns in contingency tables? Provide a clear discussion and examination of the data and the variables you are interested in using. (20/100 pts)\n",
        "\n",
        "\n",
        "### Part 2: $k$-NN classification/regression, write-up (50/100 pts)\n",
        "\n",
        "Submit a notebook that clearly addresses the following, using code and markdown chunks:\n",
        "\n",
        "1. Describe the data, particularly what an observation is and whether there are any missing data that might impact your analysis. Who collected the data and why? What known limitations are there to analysis? (10/100 pts)\n",
        "2. Describe the variables you selected to predict mortality and life expectancy, and the rationale behind them. Analyze your variables using describe tables, kernel densities, scatter plots, and conditional kernel densities. Are there any patterns of interest to notice? (10/100 pts)\n",
        "3. Using your variables to predict mortality using a $k$-Nearest Neighbor Classifier. Analyze its performance and explain clearly how you select $k$. (10/100 pts)\n",
        "4. Using your variables to predict life expectancy using a $k$-Nearest Neighbor Regressor. Analyze its performance and explain clearly how you select $k$. (10/100 pts)\n",
        "5. Describe how your model could be used for health interventions based on patient characteristics. Are there any limitations or risks to consider? (10/100 pts)\n",
        "\n",
        "## Submission (10/100 pts)\n",
        "\n",
        "Submit your work in a well-organized GitHub repo, where the code is appropriately commented and all members of the group have made significant contributions to the commit history. (10/100 pts)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#basically clones the data so the next codechunk runs\n",
        "!git clone https://github.com/ds4e/undergrad_ml_assignments/ data-real"
      ],
      "metadata": {
        "id": "7kqwQy0DtqDR",
        "outputId": "00595fa7-b884-42d8-fdfc-060fcd2f7aed",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "7kqwQy0DtqDR",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'data-real' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#taken from codechunk above\n",
        "import pandas as pd\n",
        "mdf = pd.read_csv('/content/data-real/data/linked_mortality_file_1999_2000.csv') # Load mortality file\n",
        "print( mdf.head() )\n",
        "gdf = pd.read_sas(\"/content/data-real/data/DEMO.xpt\", format=\"xport\") # Load demographics file\n",
        "print( gdf.head() )\n",
        "df = gdf.merge(mdf, on=\"SEQN\", how=\"inner\") # Merge mortality and demographics on SEQN variable\n"
      ],
      "metadata": {
        "id": "ajYdFJYTtUsX",
        "outputId": "499e452a-c3e1-491f-a543-3c10af423605",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "ajYdFJYTtUsX",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   SEQN  ELIGSTAT  MORTSTAT  UCOD_LEADING  DIABETES  HYPERTEN  PERMTH_INT  \\\n",
            "0     1         2       NaN           NaN       NaN       NaN         NaN   \n",
            "1     2         1       1.0           6.0       0.0       0.0       177.0   \n",
            "2     3         2       NaN           NaN       NaN       NaN         NaN   \n",
            "3     4         2       NaN           NaN       NaN       NaN         NaN   \n",
            "4     5         1       0.0           NaN       NaN       NaN       244.0   \n",
            "\n",
            "   PERMTH_EXM  \n",
            "0         NaN  \n",
            "1       177.0  \n",
            "2         NaN  \n",
            "3         NaN  \n",
            "4       244.0  \n",
            "   SEQN  SDDSRVYR  RIDSTATR  RIDEXMON  RIAGENDR  RIDAGEYR  RIDAGEMN  RIDAGEEX  \\\n",
            "0   1.0       1.0       2.0       2.0       2.0       2.0      29.0      31.0   \n",
            "1   2.0       1.0       2.0       2.0       1.0      77.0     926.0     926.0   \n",
            "2   3.0       1.0       2.0       1.0       2.0      10.0     125.0     126.0   \n",
            "3   4.0       1.0       2.0       2.0       1.0       1.0      22.0      23.0   \n",
            "4   5.0       1.0       2.0       2.0       1.0      49.0     597.0     597.0   \n",
            "\n",
            "   RIDRETH1  RIDRETH2  ...      WTIREP43      WTIREP44      WTIREP45  \\\n",
            "0       4.0       2.0  ...  10094.017100   9912.461855   9727.078709   \n",
            "1       3.0       1.0  ...  27186.728682  27324.345051  28099.663528   \n",
            "2       3.0       1.0  ...  43993.193099  44075.386428  46642.563799   \n",
            "3       4.0       2.0  ...  10702.307249  10531.444441  10346.119327   \n",
            "4       3.0       1.0  ...  93164.782430  92119.608772  95388.490406   \n",
            "\n",
            "       WTIREP46      WTIREP47      WTIREP48      WTIREP49      WTIREP50  \\\n",
            "0  10041.524113  9.953956e+03   9857.381983   9865.152486  10327.992682   \n",
            "1  27757.066921  2.804929e+04  26716.602006  26877.704909  27268.025234   \n",
            "2  44967.681579  4.457248e+04  44087.945688  44831.370881  44480.987235   \n",
            "3  10636.063039  5.397605e-79  10533.108939  10654.749584  10851.024385   \n",
            "4  94131.383538  9.529781e+04  91325.082461  91640.586117  92817.926915   \n",
            "\n",
            "       WTIREP51      WTIREP52  \n",
            "0   9809.165049  10323.315747  \n",
            "1  27406.383620  26984.812909  \n",
            "2  45389.112766  43781.905637  \n",
            "3  10564.981435  11012.529729  \n",
            "4  94282.855382  91993.251203  \n",
            "\n",
            "[5 rows x 144 columns]\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}